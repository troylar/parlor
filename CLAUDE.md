# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Anteroom is a self-hosted, private ChatGPT-style web UI and agentic CLI that connects to any OpenAI-compatible API. It provides two interfaces: a FastAPI web UI with vanilla JS frontend, and a Rich-based CLI REPL with built-in tools and MCP integration. Single-user, local-first, SQLite-backed.

## Development Commands

```bash
# Install for development
pip install -e ".[dev]"

# Run the app
aroom                               # Web UI at http://127.0.0.1:8080
aroom --port 9090                   # Override port (also: AI_CHAT_PORT=9090)
aroom chat                          # CLI REPL
aroom init                          # Interactive setup wizard
aroom --test                        # Validate AI connection
aroom --version                     # Show version
aroom chat --model gpt-4o           # Override model
aroom --approval-mode auto          # Override safety mode for session
aroom --allowed-tools bash,write_file       # Pre-allow tools
aroom --approval-mode auto chat     # Works with subcommands too

# Testing
pytest tests/ -v                    # All tests
pytest tests/unit/ -v               # Unit tests only
pytest tests/e2e/ -v                # E2e tests (requires uvx/npx)
pytest -m e2e -v                    # Run only e2e-marked tests
pytest tests/unit/test_tools.py -v  # Single test file
pytest tests/unit/test_tools.py::test_name -v  # Single test
pytest --cov=anteroom --cov-report=html  # With coverage

# Linting & formatting
ruff check src/ tests/              # Lint
ruff check src/ tests/ --fix        # Lint with auto-fix
ruff format src/ tests/             # Format (120 char line length)
```

## Architecture

### Dual Interface, Shared Core

Both the web UI and CLI share the same agent loop (`services/agent_loop.py`) and storage layer. This is the central design pattern — changes to tool handling, streaming, or message building affect both interfaces.

```
Web UI (routers/) ──┐
                    ├──→ agent_loop.py → ai_service.py → OpenAI-compatible API
CLI (cli/repl.py) ──┘         │
                         tools/ + mcp_manager.py
                              │
                         storage.py → SQLite (shared DB)
```

### Key Modules

- **`__main__.py`** — CLI entry point and web server launcher. Argparse-driven command dispatch (`init`, `config`, `chat`, `db` subcommands). Global flags: `--version`, `--test` (validate AI connection), `--allowed-tools` (pre-allow tools), `--approval-mode` (override safety mode), `--port` (override web server port). `_run_web()` launches uvicorn with optional TLS, validates AI connection, opens browser in a deferred daemon thread, and handles port conflicts (EADDRINUSE on any OS including macOS errno 48) with actionable error messages suggesting `--port` flag and `AI_CHAT_PORT` env var. `_run_chat()` launches the CLI REPL with optional prompt, tool disabling, conversation resume/continuation, project context, and model override
- **`app.py`** — FastAPI app factory, middleware stack (auth, rate limiting, CSRF, security headers with conditional HSTS based on TLS config, body size limit). `_derive_auth_token()` derives a stable HMAC-SHA256 token from the Ed25519 identity key so browser cookies survive server restarts; falls back to a random token when no identity is present. `ensure_identity()` is called in `create_app()` before token derivation when identity is absent or its `private_key` is missing (covers first-run and corrupted-config cases). `BearerTokenMiddleware._make_401()` attaches a fresh `anteroom_session` cookie to 401 responses so browsers auto-recover without a redirect loop
- **`config.py`** — YAML config loading with env var overrides, dataclass hierarchy (`AppConfig` → `AIConfig`, `AppSettings`, `CliConfig`, `McpServerConfig`, `SharedDatabaseConfig`, `UserIdentity`, `SafetyConfig`, `SafetyToolConfig`, `SubagentConfig`, `EmbeddingsConfig`). `AppSettings.tls` controls HTTPS, HSTS, and secure cookies. `ensure_identity()` auto-generates Ed25519 keypair on first run; also repairs partial identity (user_id present but no private_key) by generating and persisting a fresh keypair. `_DEFAULT_SYSTEM_PROMPT` provides Claude Code-caliber coding instructions across 7 XML sections (agentic_behavior, tool_use, code_modification, git_operations, investigation, communication, safety). `_BUILTIN_TOOL_DESCRIPTIONS` maps 10 built-in tools to usage-guidance descriptions. `EmbeddingsConfig` supports dual embedding providers: `provider` field (`"local"` for fastembed, `"api"` for OpenAI-compatible; default `"local"`), `local_model` (default `"BAAI/bge-small-en-v1.5"` for offline embeddings), `model` (for API provider), `dimensions` (0 = auto-detect), plus `base_url`, `api_key`, `api_key_command` for API auth
- **`identity.py`** — User identity generation: Ed25519 keypair via `cryptography`, UUID4 user IDs, PEM serialization
- **`services/agent_loop.py`** — Shared agentic loop: streams responses, parses tool calls, executes tools in parallel via `asyncio.as_completed`, loops up to `max_tool_iterations` (50). Auto-compacts when context exceeds threshold (configurable via `cli.context_auto_compact_tokens`, default 100K tokens). `_build_compaction_history()` builds structured tool-outcome summaries (name, args preview, SUCCESS/ERROR) so the AI retains proof of completed steps after compaction. Emits `"thinking"` event before every API call (including the first iteration) for UI spinners. Forwards `"phase"` and `"retrying"` events from `ai_service.stream_chat()` for lifecycle tracking. Accepts optional `message_queue` param for prompt queuing — checks queue after each `done` event and continues the loop if messages are pending. Accepts optional `narration_cadence` param (int, default 0 = disabled): after every N completed tool calls, emits a `"thinking"` event (so UI spinners restart) then injects an ephemeral `user` message (`_NARRATION_PROMPT`) to force a 1–2 sentence progress summary from the AI; the message is removed from history by index immediately after the narration response so it does not pollute subsequent tool calls or conversation context
- **`services/ai_service.py`** — OpenAI SDK wrapper with streaming and transparent token refresh on 401. Split timeout architecture: `connect_timeout` (default 5s) for TCP connect, `write_timeout` (default 30s) for sending request body, `pool_timeout` (default 10s) for waiting for free connection from httpx pool, `first_token_timeout` (default 30s) for first response token, `request_timeout` (default 120s) for overall stream, `chunk_stall_timeout` (default 30s) for max silence between chunks mid-stream — all configurable. All three phases — `create()` call, first-token wait, and mid-stream iteration — are cancel-aware via `asyncio.wait()` racing against `cancel_event` and hard `request_timeout`/`first_token_timeout` deadlines, ensuring user disconnection (Escape) interrupts at any point and httpx per-read timeouts cannot bypass the overall deadline. `_iter_stream()` provides cancel-aware async stream iteration with a hard total timeout and stall-aware timeout for mid-stream silence, preventing thinking loops from hanging indefinitely; emits distinct log messages for total deadline vs mid-stream stall. Exponential backoff retry on transient errors (`APITimeoutError`, `APIConnectionError`, `_FirstTokenTimeoutError`): configurable via `retry_max_attempts` (default 3) and `retry_backoff_base` (default 1.0s); emits `"retrying"` events with attempt/max/delay/reason for UI feedback; cancel-aware backoff sleep. Non-transient errors (`AuthenticationError`, `BadRequestError`, `RateLimitError`) are not retried. After all retries exhausted, yields specific error events based on the last transient error type. All error events include a `retryable` boolean flag (True for timeouts, connection errors, rate limits; False for auth failures, bad requests, internal errors) enabling the CLI to distinguish user-retryable failures from terminal ones. Emits `"phase"` events (`connecting` before API call, `waiting` after) for lifecycle tracking in the thinking indicator. Emits `tool_call_args_delta` events during argument accumulation for real-time canvas streaming
- **`services/event_bus.py`** — Async pub/sub event bus: in-process delivery via `asyncio.Queue` per subscriber, cross-process delivery via SQLite `change_log` polling (1.5s interval). Channels: `conversation:{id}` and `global:{db_name}`. Used by `routers/events.py` SSE endpoint for real-time UI updates
- **`services/storage.py`** — SQLite DAL with column-allowlisted SQL builder, parameterized queries, UUID-based IDs. Includes vector storage methods (`store_embedding`, `search_similar_messages`) that gracefully degrade when sqlite-vec is unavailable. Source CRUD: `create_source`, `get_source`, `list_sources`, `update_source`, `delete_source`, `save_source_file` (with MIME validation and magic byte check). Text chunking via `chunk_text()` (sentence-boundary splitting, configurable max_size/overlap). Source tags (`add_tag_to_source`, `remove_tag_from_source`), source groups (`create_source_group`, `list_source_groups`, `delete_source_group`, `add_source_to_group`, `remove_source_from_group`), project linking (`link_source_to_project`, `unlink_source_from_project`, `get_project_sources` resolving all 3 link types). Dual citizenship via `create_source_from_attachment()`. Source chunk embeddings via `store_source_chunk_embedding()` and `search_similar_source_chunks()`
- **`services/embeddings.py`** — Dual embedding service strategy: `EmbeddingService` calls OpenAI-compatible embedding APIs (with token refresh), while `LocalEmbeddingService` uses the `fastembed` library for offline-first embedding generation. Provider selection (`local` or `api`) is controlled by `EmbeddingsConfig.provider`. Default is `local` with model `BAAI/bge-small-en-v1.5`. Both services validate vectors and manage dimensions consistently. Local embeddings gracefully degrade if fastembed is unavailable
- **`services/embedding_worker.py`** — Background worker that processes unembedded messages and source chunks asynchronously with exponential backoff (2x multiplier, capped at 300s). Marks short messages (`< MIN_CONTENT_LENGTH`), None embeddings, and repeatedly-failing store operations as skipped/failed via sentinel rows in `message_embeddings`/`source_chunk_embeddings` (status column) so they are excluded from future `get_unembedded_*` queries. In-memory retry counter (`_store_failures` dict) gives up after `MAX_STORE_RETRIES` (3). Distinguishes `EmbeddingPermanentError` (auto-disables worker) from `EmbeddingTransientError` (backoff + retry). Auto-disables after `MAX_CONSECUTIVE_FAILURES` (10). Inline methods `embed_message()` and `embed_source()` catch errors gracefully, allowing the background worker to retry later. Properties expose worker state: `disabled`, `disabled_reason`, `consecutive_failures`, `current_interval`
- **`services/mcp_manager.py`** — MCP client lifecycle and tool routing. `McpManager` connects one or more MCP servers at startup (stdio or SSE transport), lists their tools, and routes `call_tool()` calls to the correct session. Each server gets its own `AsyncExitStack`; on any connection error the stack is closed to prevent leaked subprocesses or task groups. `asyncio.CancelledError` is re-raised after stack cleanup so `asyncio.wait_for` timeout semantics work correctly. `McpError` (known MCP protocol failures) is logged without traceback (`exc_info=False`); unexpected errors keep full traceback (`exc_info=True`). Supports per-server `connect_server`, `disconnect_server`, and `reconnect_server`. Returns OpenAI-format tool schemas via `get_openai_tools()`; warns on tool-name collisions across servers.
- **`routers/sources.py`** — Sources API: full CRUD for knowledge sources (`GET/POST/PATCH/DELETE /api/sources`), file upload (`POST /api/sources/upload`), tag/untag sources, source groups CRUD (`/api/source-groups`), group membership management, project-source linking (`/api/projects/{id}/sources`). UUID validation, Content-Type enforcement, Pydantic request models
- **`routers/search.py`** — Search API: `/api/search/semantic` (vector similarity) and `/api/search/hybrid` (FTS5 + vector). Both endpoints require sqlite-vec
- **`tools/`** — ToolRegistry pattern: `_handlers` (async callables) + `_definitions` (OpenAI function schemas). Built-in tools: read_file, write_file, edit_file, bash, glob_files, grep, create_canvas, update_canvas, patch_canvas, run_agent. Safety gate integration: `call_tool()` checks tier-based approval via `tools/tiers.py` + pattern detection via `tools/safety.py` + hard-block detection via `tools/security.py`, accepts per-call `confirm_callback` for interface-specific approval flows. Hard-block patterns (rm -rf, fork bombs, etc.) are checked before the approval prompt: in interactive mode the user sees an escalated "DESTRUCTIVE" warning and can approve; with no callback the command is silently blocked. When the user approves a hard-blocked command, `_bypass_hard_block=True` is passed to the bash handler to skip redundant `sanitize_command()`. Session permissions (`_session_allowed` set) and config allowlists bypass approval checks
- **`tools/tiers.py`** — Tool risk tier system (Claude Code-style). `ToolTier` enum: READ (0), WRITE (1), EXECUTE (2), DESTRUCTIVE (3). `ApprovalMode` enum: AUTO (bypass all), ASK_FOR_DANGEROUS (only destructive), ASK_FOR_WRITES (default, write+execute+destructive), ASK (same as ask_for_writes). `should_require_approval()` returns tri-state: True (needs approval), False (auto-allow), None (hard deny from denied_tools). `DEFAULT_TOOL_TIERS` maps built-in tools; unknown/MCP tools default to EXECUTE
- **`tools/safety.py`** — Pure detection logic for destructive operations. `check_bash_command()` matches against compiled regex patterns (13 defaults + configurable custom patterns). `check_write_path()` detects writes to sensitive paths (`.env`, `.ssh`, `.gnupg`, etc.). Returns `SafetyVerdict` dataclass with `is_hard_blocked` and `hard_block_description` fields for escalated hard-block warnings. No I/O, no side effects. Pattern detection runs even when tier check auto-allows (except in AUTO mode), ensuring destructive commands like `git reset --hard` always prompt
- **`tools/canvas.py`** — Canvas tools for AI to create/update rich content panels alongside chat. Supports streaming content updates during generation via SSE events (`canvas_stream_start`, `canvas_streaming`). `patch_canvas` applies incremental search/replace edits for token efficiency
- **`tools/subagent.py`** — Sub-agent tool (`run_agent`): spawns isolated child AI sessions for parallel task execution. Each sub-agent gets its own conversation context, deepcopy'd AIService config, and defensive system prompt. Sub-agents receive both built-in tools (from `ToolRegistry`) and MCP tools (from `McpManager`); the `child_tool_executor` applies the same safety gate to MCP calls as the parent executor (hard deny, approval via `_confirm_callback`, fail-closed when no callback). Guarded by `SubagentLimiter` (asyncio.Semaphore). All limits configurable via `safety.subagent` in config.yaml (`SubagentConfig` dataclass): `max_concurrent` (default 5), `max_total` (default 10), `max_depth` (default 3), `max_iterations` (default 15), `timeout` (default 120s, clamped 10-600), `max_output_chars` (default 4000), `max_prompt_chars` (default 32000). Wall-clock timeout per sub-agent via `asyncio.wait_for`. Module-level constants serve as fallback defaults when no config is provided. Generic error messages returned to parent; full traces logged server-side
- **`routers/approvals.py`** — `POST /api/approvals/{approval_id}/respond` endpoint for Web UI safety gate approval flow. Uses Pydantic request model with `scope` field (once/session/always), explicit `Content-Type: application/json` enforcement, regex-validated approval IDs, atomic dict pop to prevent TOCTOU races
- **`routers/events.py`** — `GET /events` SSE endpoint for real-time UI updates (canvas streaming, approval notifications) backed by `services/event_bus.py`
- **`cli/renderer.py`** — Rich-based terminal output for the CLI. Manages verbosity levels (`Verbosity` enum: COMPACT, NORMAL, VERBOSE), thinking spinner with background asyncio ticker task (`_thinking_ticker()`, 0.5s interval) that keeps the timer advancing even when no tokens arrive, real-time connection health monitor showing lifecycle phases (connecting, connected · waiting for first token, streaming · N chars, stalled, retry N/M) at all verbosity levels via `set_thinking_phase()`, `increment_streaming_chars()`, `_phase_suffix()` with per-phase elapsed timing (`_phase_elapsed_str()`). Async `stop_thinking()` awaits ticker task termination to prevent output races, accepts `error_msg` (pale red `ERROR_RED #CD6B6B`) and `cancel_msg` kwargs for clean inline final states. `stop_thinking_sync()` provides sync fallback for KeyboardInterrupt handlers. `thinking_countdown(delay, cancel_event, error_msg)` shows auto-retry countdown with "esc to give up" hint. `configure_thresholds()` allows runtime override of visual timing thresholds (`esc_hint_delay`, `stall_display`, `stall_warning`) from config. Tool call dedup (`set_tool_dedup()`), subagent rendering, markdown streaming, and ANSI escape code output for REPL mode (avoids Rich Status conflicts with prompt_toolkit's `patch_stdout`)
- **`cli/repl.py`** — REPL with prompt_toolkit, skills system, @file references, /commands. Uses concurrent input/output architecture with `patch_stdout()` — input prompt stays active while agent streams responses, with messages queued and processed in FIFO order. Handles `phase` events from the agent loop to update the renderer's lifecycle phase display; calls `renderer.increment_thinking_tokens()` and `renderer.increment_streaming_chars()` on token events to track streaming progress. Separates user cancel (esc → back to prompt immediately via `await renderer.stop_thinking(cancel_msg="cancelled")`) from system errors (retryable errors trigger `renderer.thinking_countdown()` with auto-retry loop up to `max_user_retries=3`, esc during countdown gives up; non-retryable errors show inline error and return to prompt). `_confirm_destructive` approval callback prints a dim collapsed summary after user responds; EOFError/KeyboardInterrupt (Escape or Ctrl+C) treated as denial — fails closed. Wires sub-agent context into tool executor (`SubagentLimiter`, `_cli_event_sink` for Rich rendering, `_subagent_counter` for unique agent IDs, `_mcp_manager` for MCP tool access in sub-agents). `_detect_project_context()` auto-detects git repo, project type (pyproject.toml, package.json, etc.), and notable directories for the system prompt. `_build_system_prompt()` assembles runtime context + project context + ANTEROOM.md instructions
- **`tls.py`** — Self-signed certificate generation for localhost HTTPS using `cryptography` package
- **`routers/`** — FastAPI endpoints: conversations CRUD, SSE chat streaming, config, projects, canvas CRUD, document/note entry management, sources CRUD with groups and project linking. Chat endpoint accepts `source_ids`, `source_tag`, `source_group_id` in request body; resolves references and injects source content into `extra_system_prompt` (50K char limit, XML-delimited with prompt injection warning). Chat endpoint supports prompt queuing: if a stream is active for a conversation, new messages are queued (max 10) and return `{"status": "queued"}` JSON instead of opening a new SSE stream. Stale stream detection checks `request.is_disconnected()` and stream age (approval_timeout + 30s); stale streams are cancelled and replaced rather than queued to. `_active_streams` stores `{started_at, request, cancel_event}` dicts. State-changing endpoints with JSON bodies enforce Content-Type validation. Chat endpoint also wires sub-agent context (`SubagentLimiter`, `_web_event_sink` buffering up to 500 events per agent, SSE `subagent_event` emission, `_mcp_manager` for MCP tool access in sub-agents)

### Security Model

Single-user local app with OWASP ASVS Level 1. Auth via HttpOnly session cookies + CSRF double-submit with Origin header validation (defense-in-depth). Auth token is stable across restarts: derived from Ed25519 private key via HMAC-SHA256 (`_derive_auth_token()`), falls back to random token when no identity exists. On 401, `_make_401()` sets a fresh session cookie in the response so browsers auto-recover without a redirect loop; partial identity configs (user_id but no private_key) are auto-repaired on startup. Security middleware in `app.py` handles: rate limiting (120 req/min), body size (15MB), security headers (CSP, HSTS, X-Frame-Options), HMAC-SHA256 token comparison, session absolute timeout. Tool safety in `tools/security.py` blocks path traversal; `check_hard_block()` detects catastrophic command patterns (rm -rf, fork bombs, disk wipes) before the approval prompt, and `sanitize_command()` remains as last-line defense at the handler level. Tool approval system (Claude Code-style): 4 risk tiers (read/write/execute/destructive), 4 approval modes (auto/ask_for_dangerous/ask_for_writes/ask), 3 permission scopes (Allow Once, Allow for Session, Allow Always). MCP tools are also gated at the `_tool_executor` level in both `chat.py` and `cli/repl.py`, and at the `child_tool_executor` level inside `tools/subagent.py` for sub-agent MCP calls. Config-based `allowed_tools` and `denied_tools` lists override tier checks. Session permissions are in-memory; "Always" permissions persist to `config.yaml` via `write_allowed_tool()`. The `tool_calls` table tracks `approval_decision` for audit. The Web UI flow uses `asyncio.Event` with disconnect-aware polling (1s interval, checks `request.is_disconnected()`, configurable timeout default 120s), event bus SSE for notifications, and `routers/approvals.py` for response handling. In-memory `pending_approvals` dict on `app.state` (capped at 100 entries). Fails closed: no approval channel = operation blocked.

### Database

SQLite with WAL journaling, FTS5 for search, foreign keys enforced. Schema defined in `db.py`. Tables: users, conversations, messages, attachments, tool_calls (with `approval_decision` audit column), projects, folders, tags, conversation_tags, message_embeddings, canvases, change_log, sources, source_chunks, source_tags, source_groups, source_group_members, project_sources (3-mode linking via CHECK constraint: source_id, group_id, or tag_filter), source_attachments (dual citizenship bridge), source_chunk_embeddings. Conversations have a `type` column (`chat`, `note`, `document`) controlling behavior. All entity tables carry `user_id` and `user_display_name` columns for identity attribution. Optional sqlite-vec extension enables vector similarity search via the `vec_messages` virtual table (created with `vec0`); `message_embeddings` is a companion regular table storing metadata (content hash, chunk index). Vector schema is parameterized via `_make_vec_schema(dimensions)` to support different embedding dimensions; the table is recreated if dimensions change between app runs via `_ensure_vec_schema_matches_config()`.

### Configuration

Config file at `~/.anteroom/config.yaml` (falls back to `~/.parlor/config.yaml` for backward compat). Environment variables override config values with `AI_CHAT_` prefix (e.g., `AI_CHAT_BASE_URL`, `AI_CHAT_API_KEY`, `AI_CHAT_MODEL`, `AI_CHAT_PORT`, `AI_CHAT_USER_ID`, `AI_CHAT_DISPLAY_NAME`). Token provider pattern (`api_key_command`) enables dynamic API key refresh via external commands. TLS is disabled by default (`app.tls: false`); set to `true` to enable HTTPS with a self-signed certificate. User identity (Ed25519 keypair + UUID) is auto-generated on first run and stored in the `identity` config section. `AIConfig` timeout fields: `connect_timeout` (default 5s, env: `AI_CHAT_CONNECT_TIMEOUT`, clamped 1–30) for TCP connect, `write_timeout` (default 30s, env: `AI_CHAT_WRITE_TIMEOUT`, clamped 5–120) for sending request body, `pool_timeout` (default 10s, env: `AI_CHAT_POOL_TIMEOUT`, clamped 1–60) for waiting for free httpx connection, `first_token_timeout` (default 30s, env: `AI_CHAT_FIRST_TOKEN_TIMEOUT`, clamped 5–120) for max wait before first token after connect, `request_timeout` (default 120s, env: `AI_CHAT_REQUEST_TIMEOUT`, clamped 10–600) for overall stream timeout, `chunk_stall_timeout` (default 30s, env: `AI_CHAT_CHUNK_STALL_TIMEOUT`, clamped 10–600) for max silence between chunks mid-stream. Retry fields: `retry_max_attempts` (default 3, env: `AI_CHAT_RETRY_MAX_ATTEMPTS`, clamped 0–10) for transient error retries (0 disables), `retry_backoff_base` (default 1.0s, env: `AI_CHAT_RETRY_BACKOFF_BASE`, clamped 0.1–30.0) for exponential backoff base delay. `AIConfig.narration_cadence` (default 5, env: `AI_CHAT_NARRATION_CADENCE`) sets the progress update frequency during multi-step tasks: every N tool calls the AI emits a 1–2 sentence summary of findings and next steps; 0 disables narration. `EmbeddingsConfig` controls vector embeddings: `enabled` (default true), `provider` (default `"local"` for offline-first fastembed; set to `"api"` for OpenAI-compatible), `local_model` (default `"BAAI/bge-small-en-v1.5"` for offline embeddings), `model` (for API provider), `dimensions` (0 = auto-detect), `base_url`, `api_key`, `api_key_command` (for API auth). Local embeddings run offline; API embeddings require network access. `SafetyConfig` controls tool safety gates: `enabled` flag, `approval_mode` (auto/ask_for_dangerous/ask_for_writes/ask, default ask_for_writes, env: `AI_CHAT_SAFETY_APPROVAL_MODE`), `approval_timeout` (seconds, default 120, clamped 10–600), per-tool `SafetyToolConfig` entries with `enabled` boolean, `allowed_tools` (list, always auto-approved), `denied_tools` (list, hard-blocked), `tool_tiers` (dict mapping tool names to tier strings for overrides). Global `custom_patterns` (list of regex strings for bash) and `sensitive_paths` (list of path strings for write_file) are top-level fields. `CliConfig` controls CLI-specific behavior: `context_warn_tokens` (default 80K) and `context_auto_compact_tokens` (default 100K) set the thresholds for context warnings and auto-compaction. `tool_dedup` (default true, env: `AI_CHAT_TOOL_DEDUP`) controls whether consecutive similar tool calls are collapsed in CLI output; groups by tool action type (e.g. multiple edits to different files show as one line with a count). CLI retry behavior: `retry_delay` (default 5.0s, clamped 1–60) controls countdown tick delay, `max_retries` (default 3, clamped 0–10) limits auto-retry attempts for retryable errors. Visual thresholds: `esc_hint_delay` (default 3.0s) before showing "esc to cancel", `stall_display_threshold` (default 5.0s, clamped 1+) before showing stalled indicator, `stall_warning_threshold` (default 15.0s, clamped 1+) before showing full warning. Output limits: `tool_output_max_chars` (default 2000, clamped 100+) per tool result, `file_reference_max_chars` (default 100K, clamped 1000+) from @file references, `model_context_window` (default 128K, clamped 1000+) for usage bar calculations. `SubagentConfig` (nested under `safety.subagent`) controls sub-agent limits: `max_concurrent` (default 5), `max_total` (default 10), `max_depth` (default 3), `max_iterations` (default 15), `timeout` (seconds, default 120, clamped 10–600), `max_output_chars` (default 4000), `max_prompt_chars` (default 32000). All fields optional with sensible defaults.

### Developer Workflow

This project uses Claude Code skills (`.claude/commands/`) and auto-loaded rules (`.claude/rules/`) to enforce development standards. See `VISION.md` for product identity and scope guardrails. See `ROADMAP.md` for the prioritized roadmap organized by VISION.md direction areas.

**Skills** (invoke with `/command`): `/ideate`, `/new-issue`, `/start-work`, `/commit`, `/submit-pr`, `/pr-check`, `/code-review`, `/deploy`, `/write-docs`, `/dev-help`, `/next`, `/triage`, `/cleanup`. Run `/dev-help` for a full guide.

**Rules** (auto-loaded every session): commit format, issue requirement, output formatting, product vision alignment, security patterns, test requirements.

### Deployment

PyPI package: `anteroom`. Deploy via `/deploy` Claude Code skill which handles: merge PR, wait for CI, version bump, build, and `twine upload`. Requires `build` and `twine` installed. Credentials via `~/.pypirc` or `TWINE_USERNAME`/`TWINE_PASSWORD` env vars.

## Testing Patterns

- **Async tests** use `@pytest.mark.asyncio` with `asyncio_mode = "auto"` in pyproject.toml
- **Unit tests** are fully mocked (no I/O), integration tests use real SQLite databases
- **E2e tests** start real servers with real MCP servers but mock the AI service. Require `uvx` and/or `npx` on PATH; tests skip gracefully when unavailable
- Tests are in `tests/unit/`, `tests/integration/`, `tests/contract/`, `tests/e2e/`
- **Pytest markers**: `e2e` (end-to-end tests requiring real services), `real_ai` (tests that call a real AI backend, require API key)
- Coverage target: 80%+

## CI

GitHub Actions (`.github/workflows/test.yml`): test matrix across Python 3.10-3.14, ruff lint+format check, pytest with coverage, pip-audit, Snyk SCA (enforced, production deps only) + SAST (informational, SARIF uploaded for visibility but non-blocking due to false positives in taint analysis).
